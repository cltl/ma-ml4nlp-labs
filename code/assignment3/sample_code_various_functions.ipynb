{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.2\n",
    "\n",
    "## Goal\n",
    "\n",
    "The performance of machine learning systems directly depends on the quality of input features. In this exercise, you will investigate the impact of individual features on a system for named entity recognition: what does the inclusion of each individual feature do to the results? And what happens when they are combined?\n",
    "\n",
    "\n",
    "\n",
    "## Acknowledgement\n",
    "\n",
    "This exercise made use of examples from the following exercise (in the HLT course):\n",
    "\n",
    "https://github.com/cltl/ma-hlt-labs/\n",
    "\n",
    "Lab3: machine learning\n",
    "\n",
    "\n",
    "## Procedure\n",
    "\n",
    "This notebook will provide the code for running the experiments outlined above. You will only need to make minor adaptations to run the feature ablation analysis. This notebook was developed for another more introductory course where students did not need to generate their own features. Please take that into account while reading this code (i.e. you can use this as an example, but it will not work one-to-one on your own data).\n",
    "\n",
    "The notebooks and set up have been designed for educational purposes: design choices are based on clearly illustrating what is going on and facilitating the exercises.\n",
    "\n",
    "## The Data\n",
    "\n",
    "The data of the original assignment been preprocessed to make some useful features directly availabe (as you were recommended to do as well in Assignment 2).\n",
    "\n",
    "The format of the conll files provided to the students in this original exercise was:\n",
    "\n",
    "Token  Preceding_token  Capitalization  POS-tag  Chunklabel  Goldlabel\n",
    "\n",
    "The first lines look like this:\n",
    "\n",
    "-DOCSTART-      FULLCAP -X-     -X-     O\n",
    "  \n",
    "EU              FULLCAP NNP     B-NP    B-ORG\n",
    "\n",
    "rejects EU      LOWCASE VBZ     B-VP    O\n",
    "\n",
    "Preceding_token: \n",
    "This column provides the token preceding the current token. (This is an empty space if there is no previous token).\n",
    "\n",
    "Capitalization: \n",
    "This column provides information on the capitalization of the token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "We will make use of the following packages:\n",
    "\n",
    "* scikit-learn : provides lots of useful implementations for machine learning (and has relatively good documentation!)\n",
    "* csv: a light-weight package to deal with data represented in csv (or related formats such as tsv)\n",
    "* gensim: a useful package for working with word embeddings\n",
    "* numpy: a packages that (among others) provides useful datastructures and operations to work with vectors\n",
    "\n",
    "Some notes on design decisions (feel free to ignore these if this is all new to you):\n",
    "\n",
    "* We are using csv rather than (the more common) pandas for working with the conll files, because pandas standardly applies type conversion, which we do not want when dealing with text that contains numbers (fixing this will make the code look more complex).\n",
    "* scikit-learn provides several machine learning algorithms, but this is not the focus of this exercise. We are using logistic regression, because it serves the purpose of our experiments and is relatively efficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell imports all the modules we'll need. Make sure to run this once before running the other cells\n",
    "\n",
    "\n",
    "#sklearn is scikit-learn\n",
    "import sklearn\n",
    "import csv\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Traditional Features\n",
    "\n",
    "In this first part, we will explore the impact of various features on named entity recognition.\n",
    "We will use so-called traditional features, where the feature values (strings) are presented by one-hot encoding \n",
    "\n",
    "## Step 1: A Basic Classifier\n",
    "\n",
    "We will first walk through the process of creating and evaluating a simple classifier that only uses the token itself as a feature. In the next step, we will run evaluations on this basic system.\n",
    "\n",
    "This is generally a good way to start experimenting: first walk through the entire experimental process with a very basic, easy to create system to see if everything works, there are no problems with the data etc. You can then build up from there towards a more sophististicated system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for feature extraction and training a classifier\n",
    "\n",
    "\n",
    "## For documentation on how to create input representations of features in scikit-learn:\n",
    "# https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "#Setting some variables that we will use multiple times\n",
    "trainfile = '../data/conll_train_cap.txt'\n",
    "testfile = '../data/conll_test_cap.txt'\n",
    "\n",
    "def extract_features_token_only_and_labels(conllfile):\n",
    "    '''Function that extracts features and gold label from preprocessed conll (here: tokens only).\n",
    "    \n",
    "    :param conllfile: path to the (preprocessed) conll file\n",
    "    :type conllfile: string\n",
    "    \n",
    "    \n",
    "    :return features: a list of dictionaries, with key-value pair providing the value for the feature `token' for individual instances\n",
    "    :return labels: a list of gold labels of individual instances\n",
    "    '''\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    conllinput = open(conllfile, 'r')\n",
    "    #delimiter indicates we are working with a tab separated value (default is comma)\n",
    "    #quotechar has as default value '\"', which is used to indicate the borders of a cell containing longer pieces of text\n",
    "    #in this file, we have only one token as text, but this token can be '\"', which then messes up the format. We set quotechar to a character that does not occur in our file\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        #I preprocessed the file so that all rows with instances should contain 6 values, the others are empty lines indicating the beginning of a sentence\n",
    "        if len(row) == 6:\n",
    "            #structuring feature value pairs as key-value pairs in a dictionary\n",
    "            #the first column in the conll file represents tokens\n",
    "            feature_value = {'Token': row[0]}\n",
    "            features.append(feature_value)\n",
    "            #The last column provides the gold label (= the correct answer). \n",
    "            labels.append(row[-1])\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "\n",
    "\n",
    "def create_vectorizer_and_classifier(features, labels):\n",
    "    '''\n",
    "    Function that takes feature-value pairs and gold labels as input and trains a logistic regression classifier\n",
    "    \n",
    "    :param features: feature-value pairs\n",
    "    :param labels: gold labels\n",
    "    :type features: a list of dictionaries\n",
    "    :type labels: a list of strings\n",
    "    \n",
    "    :return lr_classifier: a trained LogisticRegression classifier\n",
    "    :return vec: a DictVectorizer to which the feature values are fitted. \n",
    "    '''\n",
    "    \n",
    "    vec = DictVectorizer()\n",
    "    #fit creates a mapping between observed feature values and dimensions in a one-hot vector, transform represents the current values as a vector \n",
    "    tokens_vectorized = vec.fit_transform(features)\n",
    "    lr_classifier = LogisticRegression(solver='saga')\n",
    "    lr_classifier.fit(tokens_vectorized, labels)\n",
    "    \n",
    "    return lr_classifier, vec\n",
    "\n",
    "#extract features and labels:\n",
    "feature_values, labels = extract_features_token_only_and_labels(trainfile) \n",
    "#create vectorizer and trained classifier:\n",
    "lr_classifier, vectorizer = create_vectorizer_and_classifier(feature_values, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Evaluation\n",
    "\n",
    "We will now run a basic evaluation of the system on a test file. \n",
    "Two important properties of the test file:\n",
    "\n",
    "1. the test file and training file are independent sets (if they contain identical examples, this is coincidental)\n",
    "2. the test file is preprocessed in the exact same way as the training file \n",
    "\n",
    "The first function runs our classifier on the test data.\n",
    "\n",
    "The second function prints out a confusion matrix (comparing predictions and gold labels per class). \n",
    "You can find more information on confusion matrices here: https://www.geeksforgeeks.org/confusion-matrix-machine-learning/\n",
    "\n",
    "The third function prints out the macro precision, recall and f-score of the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_predicted_and_gold_labels_token_only(testfile, vectorizer, classifier):\n",
    "    '''\n",
    "    Function that extracts features and runs classifier on a test file returning predicted and gold labels\n",
    "    \n",
    "    :param testfile: path to the (preprocessed) test file\n",
    "    :param vectorizer: vectorizer in which the mapping between feature values and dimensions is stored\n",
    "    :param classifier: the trained classifier\n",
    "    :type testfile: string\n",
    "    :type vectorizer: DictVectorizer\n",
    "    :type classifier: LogisticRegression()\n",
    "    \n",
    "    \n",
    "    \n",
    "    :return predictions: list of output labels provided by the classifier on the test file\n",
    "    :return goldlabels: list of gold labels as included in the test file\n",
    "    '''\n",
    "    \n",
    "    #we use the same function as above (guarantees features have the same name and form)\n",
    "    sparse_feature_reps, goldlabels = extract_features_token_only_and_labels(testfile)\n",
    "    #we need to use the same fitting as before, so now we only transform the current features according to this mapping (using only transform)\n",
    "    test_features_vectorized = vectorizer.transform(sparse_feature_reps)\n",
    "    predictions = classifier.predict(test_features_vectorized)\n",
    "    \n",
    "    return predictions, goldlabels\n",
    "\n",
    "def print_confusion_matrix(predictions, goldlabels):\n",
    "    '''\n",
    "    Function that prints out a confusion matrix\n",
    "    \n",
    "    :param predictions: predicted labels\n",
    "    :param goldlabels: gold standard labels\n",
    "    :type predictions, goldlabels: list of strings\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    #based on example from https://datatofish.com/confusion-matrix-python/ \n",
    "    data = {'Gold':    goldlabels, 'Predicted': predictions    }\n",
    "    df = pd.DataFrame(data, columns=['Gold','Predicted'])\n",
    "\n",
    "    confusion_matrix = pd.crosstab(df['Gold'], df['Predicted'], rownames=['Gold'], colnames=['Predicted'])\n",
    "    print (confusion_matrix)\n",
    "\n",
    "\n",
    "def print_precision_recall_fscore(predictions, goldlabels):\n",
    "    '''\n",
    "    Function that prints out precision, recall and f-score\n",
    "    \n",
    "    :param predictions: predicted output by classifier\n",
    "    :param goldlabels: original gold labels\n",
    "    :type predictions, goldlabels: list of strings\n",
    "    '''\n",
    "    \n",
    "    precision = metrics.precision_score(y_true=goldlabels,\n",
    "                        y_pred=predictions,\n",
    "                        average='macro')\n",
    "\n",
    "    recall = metrics.recall_score(y_true=goldlabels,\n",
    "                     y_pred=predictions,\n",
    "                     average='macro')\n",
    "\n",
    "\n",
    "    fscore = metrics.f1_score(y_true=goldlabels,\n",
    "                 y_pred=predictions,\n",
    "                 average='macro')\n",
    "\n",
    "    print('P:', precision, 'R:', recall, 'F1:', fscore)\n",
    "    \n",
    "#vectorizer and lr_classifier are the vectorizer and classifiers created in the previous cell.\n",
    "#it is important that the same vectorizer is used for both training and testing: they should use the same mapping from values to dimensions\n",
    "predictions, goldlabels = get_predicted_and_gold_labels_token_only(testfile, vectorizer, lr_classifier)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: A More Elaborate System\n",
    "\n",
    "Now that we have run a basic experiment, we are going to investigate alternatives. In this exercise, we only focus on features. We will continue to use the same logistic regression classifier throughout the exercise.\n",
    "\n",
    "We want to investigate the impact of individual features. We will thus use a function that allows us to specify whether we include a specific feature or not. The features we have at our disposal are:\n",
    "\n",
    "* the token itself (as used above)\n",
    "* the preceding token\n",
    "* the capitalization indication (see above for values that this takes)\n",
    "* the pos-tag of the token\n",
    "* the chunklabel of the chunk the token is part of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the functions with multiple features and analysis\n",
    "\n",
    "#defines the column in which each feature is located (note: you can also define headers and use csv.DictReader)\n",
    "feature_to_index = {'Token': 0, 'Prevtoken': 1, 'Cap': 2, 'Pos': 3, 'Chunklabel': 4}\n",
    "\n",
    "\n",
    "def extract_features_and_gold_labels(conllfile, selected_features):\n",
    "    '''Function that extracts features and gold label from preprocessed conll (here: tokens only).\n",
    "    \n",
    "    :param conllfile: path to the (preprocessed) conll file\n",
    "    :type conllfile: string\n",
    "    \n",
    "    \n",
    "    :return features: a list of dictionaries, with key-value pair providing the value for the feature `token' for individual instances\n",
    "    :return labels: a list of gold labels of individual instances\n",
    "    '''\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    conllinput = open(conllfile, 'r')\n",
    "    #delimiter indicates we are working with a tab separated value (default is comma)\n",
    "    #quotechar has as default value '\"', which is used to indicate the borders of a cell containing longer pieces of text\n",
    "    #in this file, we have only one token as text, but this token can be '\"', which then messes up the format. We set quotechar to a character that does not occur in our file\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        #I preprocessed the file so that all rows with instances should contain 6 values, the others are empty lines indicating the beginning of a sentence\n",
    "        if len(row) == 6:\n",
    "            #structuring feature value pairs as key-value pairs in a dictionary\n",
    "            #the first column in the conll file represents tokens\n",
    "            feature_value = {}\n",
    "            for feature_name in selected_features:\n",
    "                row_index = feature_to_index.get(feature_name)\n",
    "                feature_value[feature_name] = row[row_index]\n",
    "            features.append(feature_value)\n",
    "            #The last column provides the gold label (= the correct answer). \n",
    "            labels.append(row[-1])\n",
    "    return features, labels\n",
    "\n",
    "def get_predicted_and_gold_labels(testfile, vectorizer, classifier, selected_features):\n",
    "    '''\n",
    "    Function that extracts features and runs classifier on a test file returning predicted and gold labels\n",
    "    \n",
    "    :param testfile: path to the (preprocessed) test file\n",
    "    :param vectorizer: vectorizer in which the mapping between feature values and dimensions is stored\n",
    "    :param classifier: the trained classifier\n",
    "    :type testfile: string\n",
    "    :type vectorizer: DictVectorizer\n",
    "    :type classifier: LogisticRegression()\n",
    "    \n",
    "    \n",
    "    \n",
    "    :return predictions: list of output labels provided by the classifier on the test file\n",
    "    :return goldlabels: list of gold labels as included in the test file\n",
    "    '''\n",
    "    \n",
    "    #we use the same function as above (guarantees features have the same name and form)\n",
    "    features, goldlabels = extract_features_and_gold_labels(testfile, selected_features)\n",
    "    #we need to use the same fitting as before, so now we only transform the current features according to this mapping (using only transform)\n",
    "    test_features_vectorized = vectorizer.transform(features)\n",
    "    predictions = classifier.predict(test_features_vectorized)\n",
    "    \n",
    "    return predictions, goldlabels\n",
    "\n",
    "#define which from the available features will be used (names must match key names of dictionary feature_to_index)\n",
    "all_features = ['Token','Prevtoken','Cap','Pos','Chunklabel']\n",
    "\n",
    "sparse_feature_reps, labels = extract_features_and_gold_labels(trainfile, all_features)\n",
    "#we can use the same function as before for creating the classifier and vectorizer\n",
    "lr_classifier, vectorizer = create_vectorizer_and_classifier(sparse_feature_reps, labels)\n",
    "#when applying our model to new data, we need to use the same features\n",
    "predictions, goldlabels = get_predicted_and_gold_labels(testfile, vectorizer, lr_classifier, all_features)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Ablation Analysis\n",
    "\n",
    "If all worked well, the system that made use of all features worked better than the system with just the tokens.\n",
    "We now want to know which of the features contributed to this improved: do we want to include all features?\n",
    "Or just some?\n",
    "\n",
    "We can investigate this using *feature ablation analysis*. This means that we systematically test what happens if we add or remove a specific feature. Ideally, we investigate all possible combinations.\n",
    "\n",
    "The cell below illustrates how you can use the code above to investigate a system with three features. You can modify the selected features to try out different combinations. You can either do this manually and rerun the cell or write a function that creates list of all combinations you want to tests and runs them one after the other.\n",
    "\n",
    "Include your results in the report of this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of system with just one additional feature\n",
    "#define which from the available features will be used (names must match key names of dictionary feature_to_index)\n",
    "selected_features = ['Token','Prevtoken','Pos']\n",
    "\n",
    "feature_values, labels = extract_features_and_gold_labels(trainfile, selected_features)\n",
    "#we can use the same function as before for creating the classifier and vectorizer\n",
    "lr_classifier, vectorizer = create_vectorizer_and_classifier(feature_values, labels)\n",
    "#when applying our model to new data, we need to use the same features\n",
    "predictions, goldlabels = get_predicted_and_gold_labels(testfile, vectorizer, lr_classifier, selected_features)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predictions, goldlabels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: One-hot versus Embeddings\n",
    "\n",
    "In this second part of the exercise, we will compare results using one-hot encodings to pretrained word embeddings.\n",
    "\n",
    "## One-hot representation\n",
    "\n",
    "In one-hot representation, each feature value is represented by an n-dimensional vector, where n corresponds to the number of possible values the feature can take. In our system, the Token feature can take the value of each token that occurs at least once in the corpus. This feature thus uses a vector with the size of the vocabulary in the corpus. Each possible value is associated with a specific dimension. If this value is represented, that dimension will receive the value 1 and all other dimensions will have the value 0.\n",
    "\n",
    "The system receive a concatenation of all feature representations as input.\n",
    "\n",
    "\n",
    "## What does one-hot look like?\n",
    "\n",
    "We will start with an illustration of a one-hot representation. We will use the capitalization feature for this: it has 6 possible values and is therefore represented by a 6-dimensional vector. If you would like a more precise look, you may consider creating a toy example of a few lines, in which the capitalization feature has different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classifier with caps feature only and print vectorizer, then with token only (but you see less)\n",
    "\n",
    "selected_features = ['Cap']\n",
    "\n",
    "feature_values, labels = extract_features_and_gold_labels(trainfile, selected_features)\n",
    "\n",
    "#creating a vectorizing\n",
    "vectorizer = DictVectorizer()\n",
    "#fitting the values to dimensions (creating a mapping) and transforming the current observations according to this mapping\n",
    "capitalization_vectorized = vectorizer.fit_transform(feature_values)\n",
    "print(capitalization_vectorized.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word embeddings\n",
    "\n",
    "We are now going to use word embeddings to represent tokens. We load a pretrained distributional semantic model.\n",
    "You can use the same model as in Exercise 2.1. We tested the exercise with the same model (GoogleNews negative sampling 300 dimensions) as Exercise 2.1 as well.\n",
    "\n",
    "Note: loading the model may take a while. You probably want to run that only once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step takes a while\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('../models/GoogleNews-vectors-negative300.bin.gz', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_as_features_and_gold(conllfile,word_embedding_model):\n",
    "    '''\n",
    "    Function that extracts features and gold labels using word embeddings\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if len(row) == 6:\n",
    "            if row[0] in word_embedding_model:\n",
    "                vector = word_embedding_model[row[0]]\n",
    "            else:\n",
    "                vector = [0]*300\n",
    "            features.append(vector)\n",
    "            labels.append(row[-1])\n",
    "    return features, labels\n",
    "\n",
    "def create_classifier(features, labels):\n",
    "    '''\n",
    "    Function that creates classifier from features represented as vectors and gold labels\n",
    "    \n",
    "    :param features: list of vector representations of tokens\n",
    "    :param labels: list of gold labels\n",
    "    :type features: list of vectors\n",
    "    :type labels: list of strings\n",
    "    \n",
    "    :returns trained logistic regression classifier\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    lr_classifier = LogisticRegression(solver='saga')\n",
    "    lr_classifier.fit(features, labels)\n",
    "    \n",
    "    return lr_classifier\n",
    "    \n",
    "    \n",
    "def label_data_using_word_embeddings(testfile, word_embedding_model, classifier):\n",
    "    '''\n",
    "    Function that extracts word embeddings as features and gold labels from test data and runs a classifier\n",
    "    \n",
    "    :param testfile: path to test file\n",
    "    :param word_embedding_model: distributional semantic model\n",
    "    :param classifier: trained classifier\n",
    "    :type testfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    :type classifier: LogisticRegression\n",
    "    \n",
    "    :return predictions: list of predicted labels\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    \n",
    "    dense_feature_representations, labels = extract_embeddings_as_features_and_gold(testfile,word_embedding_model)\n",
    "    predictions = classifier.predict(dense_feature_representations)\n",
    "    \n",
    "    return predictions, labels\n",
    "\n",
    "\n",
    "# I printing announcements of where the code is at (since some of these steps take a while)\n",
    "\n",
    "print('Extracting dense features...')\n",
    "dense_feature_representations, labels = extract_embeddings_as_features_and_gold(trainfile,word_embedding_model)\n",
    "print('Training classifier....')\n",
    "classifier = create_classifier(dense_feature_representations, labels)\n",
    "print('Running evaluation...')\n",
    "predicted, gold = label_data_using_word_embeddings(testfile, word_embedding_model, classifier)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predicted, gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including the preceding token\n",
    "\n",
    "We can include the preceding token as a feature in a similar way. We simply concatenate the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_embeddings_of_current_and_preceding_as_features_and_gold(conllfile,word_embedding_model):\n",
    "    '''\n",
    "    Function that extracts features and gold labels using word embeddings for current and preceding token\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if len(row) == 6:\n",
    "            if row[0] in word_embedding_model:\n",
    "                vector1 = word_embedding_model[row[0]]\n",
    "            else:\n",
    "                vector1 = [0]*300\n",
    "            if row[1] in word_embedding_model:\n",
    "                vector2 = word_embedding_model[row[1]]\n",
    "            else:\n",
    "                vector2 = [0]*300\n",
    "            features.append(np.concatenate((vector1,vector2)))\n",
    "            labels.append(row[-1])\n",
    "    return features, labels\n",
    "    \n",
    "    \n",
    "def label_data_using_word_embeddings_current_and_preceding(testfile, word_embedding_model, classifier):\n",
    "    '''\n",
    "    Function that extracts word embeddings as features (of current and preceding token) and gold labels from test data and runs a trained classifier\n",
    "    \n",
    "    :param testfile: path to test file\n",
    "    :param word_embedding_model: distributional semantic model\n",
    "    :param classifier: trained classifier\n",
    "    :type testfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    :type classifier: LogisticRegression\n",
    "    \n",
    "    :return predictions: list of predicted labels\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    \n",
    "    features, labels = extract_embeddings_of_current_and_preceding_as_features_and_gold(testfile,word_embedding_model)\n",
    "    predictions = classifier.predict(features)\n",
    "    \n",
    "    return predictions, labels\n",
    "\n",
    "\n",
    "\n",
    "print('Extracting dense features...')\n",
    "features, labels = extract_embeddings_of_current_and_preceding_as_features_and_gold(trainfile,word_embedding_model)\n",
    "print('Training classifier...')\n",
    "#we can use the same function as for just the tokens itself\n",
    "classifier = create_classifier(features, labels)\n",
    "print('Running evaluation...')\n",
    "predicted, gold = label_data_using_word_embeddings_current_and_preceding(testfile, word_embedding_model, classifier)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predicted, gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A mixed system\n",
    "\n",
    "The code below combines traditional features with word embeddings. Note that we only include features with a limited range of possible values. Combining one-hot token representations (using highly sparse dimensions) with dense representations is generally not a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_word_embedding(token, word_embedding_model):\n",
    "    '''\n",
    "    Function that returns the word embedding for a given token out of a distributional semantic model and a 300-dimension vector of 0s otherwise\n",
    "    \n",
    "    :param token: the token\n",
    "    :param word_embedding_model: the distributional semantic model\n",
    "    :type token: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :returns a vector representation of the token\n",
    "    '''\n",
    "    if token in word_embedding_model:\n",
    "        vector = word_embedding_model[token]\n",
    "    else:\n",
    "        vector = [0]*300\n",
    "    return vector\n",
    "\n",
    "\n",
    "def extract_feature_values(row, selected_features):\n",
    "    '''\n",
    "    Function that extracts feature value pairs from row\n",
    "    \n",
    "    :param row: row from conll file\n",
    "    :param selected_features: list of selected features\n",
    "    :type row: string\n",
    "    :type selected_features: list of strings\n",
    "    \n",
    "    :returns: dictionary of feature value pairs\n",
    "    '''\n",
    "    feature_values = {}\n",
    "    for feature_name in selected_features:\n",
    "        r_index = feature_to_index.get(feature_name)\n",
    "        feature_values[feature_name] = row[r_index]\n",
    "        \n",
    "    return feature_values\n",
    "    \n",
    "    \n",
    "def create_vectorizer_traditional_features(feature_values):\n",
    "    '''\n",
    "    Function that creates vectorizer for set of feature values\n",
    "    \n",
    "    :param feature_values: list of dictionaries containing feature-value pairs\n",
    "    :type feature_values: list of dictionairies (key and values are strings)\n",
    "    \n",
    "    :returns: vectorizer with feature values fitted\n",
    "    '''\n",
    "    vectorizer = DictVectorizer()\n",
    "    vectorizer.fit(feature_values)\n",
    "    \n",
    "    return vectorizer\n",
    "        \n",
    "    \n",
    "def combine_sparse_and_dense_features(dense_vectors, sparse_features):\n",
    "    '''\n",
    "    Function that takes sparse and dense feature representations and appends their vector representation\n",
    "    \n",
    "    :param dense_vectors: list of dense vector representations\n",
    "    :param sparse_features: list of sparse vector representations\n",
    "    :type dense_vector: list of arrays\n",
    "    :type sparse_features: list of lists\n",
    "    \n",
    "    :returns: list of arrays in which sparse and dense vectors are concatenated\n",
    "    '''\n",
    "    \n",
    "    combined_vectors = []\n",
    "    sparse_vectors = np.array(sparse_features.toarray())\n",
    "    \n",
    "    for index, vector in enumerate(sparse_vectors):\n",
    "        combined_vector = np.concatenate((vector,dense_vectors[index]))\n",
    "        combined_vectors.append(combined_vector)\n",
    "    return combined_vectors\n",
    "    \n",
    "\n",
    "def extract_traditional_features_and_embeddings_plus_gold_labels(conllfile, word_embedding_model, vectorizer=None):\n",
    "    '''\n",
    "    Function that extracts traditional features as well as embeddings and gold labels using word embeddings for current and preceding token\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    dense_vectors = []\n",
    "    traditional_features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if len(row) == 6:\n",
    "            token_vector = extract_word_embedding(row[0], word_embedding_model)\n",
    "            pt_vector = extract_word_embedding(row[1], word_embedding_model)\n",
    "            dense_vectors.append(np.concatenate((token_vector,pt_vector)))\n",
    "            #mixing very sparse representations (for one-hot tokens) and dense representations is a bad idea\n",
    "            #we thus only use other features with limited values\n",
    "            other_features = extract_feature_values(row, ['Cap','Pos','Chunklabel'])\n",
    "            traditional_features.append(other_features)\n",
    "            #adding gold label to labels\n",
    "            labels.append(row[-1])\n",
    "            \n",
    "    #create vector representation of traditional features\n",
    "    if vectorizer is None:\n",
    "        #creates vectorizer that provides mapping (only if not created earlier)\n",
    "        vectorizer = create_vectorizer_traditional_features(traditional_features)\n",
    "    sparse_features = vectorizer.transform(traditional_features)\n",
    "    combined_vectors = combine_sparse_and_dense_features(dense_vectors, sparse_features)\n",
    "    \n",
    "    return combined_vectors, vectorizer, labels\n",
    "\n",
    "def label_data_with_combined_features(testfile, classifier, vectorizer, word_embedding_model):\n",
    "    '''\n",
    "    Function that labels data with model using both sparse and dense features\n",
    "    '''\n",
    "    feature_vectors, vectorizer, goldlabels = extract_traditional_features_and_embeddings_plus_gold_labels(testfile, word_embedding_model, vectorizer)\n",
    "    predictions = classifier.predict(feature_vectors)\n",
    "    \n",
    "    return predictions, goldlabels\n",
    "\n",
    "\n",
    "print('Extracting Features...')\n",
    "feature_vectors, vectorizer, gold_labels = extract_traditional_features_and_embeddings_plus_gold_labels(trainfile, word_embedding_model)\n",
    "print('Training classifier....')\n",
    "lr_classifier = create_classifier(feature_vectors, gold_labels)\n",
    "print('Running the evaluation...')\n",
    "predictions, goldlabels = label_data_with_combined_features(testfile, lr_classifier, vectorizer, word_embedding_model)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predictions, goldlabels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
