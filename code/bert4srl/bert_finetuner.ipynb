{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune BERT for Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WARNING: Do NOT run this cell, unless you are running this on Google Colab. For a local installation run: pip install -r requirements.txt inside the terminal\n",
    "# % pip install transformers==4.9.1\n",
    "# % pip install datasets==1.11.0\n",
    "# % pip install tabulate==0.8.9\n",
    "# % pip install seqeval==1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, time, os\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import logging, sys\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Our code behind the scenes!\n",
    "import bert_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "BERT_MODEL_NAME = \"bert-base-cased\"\n",
    "GPU_RUN_IX=0\n",
    "\n",
    "SEED_VAL = 1234500\n",
    "SEQ_MAX_LEN = 256\n",
    "PRINT_INFO_EVERY = 10 # Print status only every X batches\n",
    "GRADIENT_CLIP = 1.0\n",
    "LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "TRAIN_DATA_PATH = \"data/trial_mini_data.conll\" # \"data/conll2003.train.conll\"\n",
    "\n",
    "# IMPORTANT NOTE: We use as validation set the test portion, in order to avoid overfitting on the dev set, \n",
    "# and this way be able to evaluate later and compare with your previous models!\n",
    "DEV_DATA_PATH = \"data/trial_mini_data.conll\" # \"data/conll2003.dev.conll\"\n",
    "\n",
    "SAVE_MODEL_DIR = \"saved_models/MY_BERT_NER/\"\n",
    "\n",
    "LABELS_FILENAME = f\"{SAVE_MODEL_DIR}/label2index.json\"\n",
    "LOSS_TRN_FILENAME = f\"{SAVE_MODEL_DIR}/Losses_Train_{EPOCHS}.json\"\n",
    "LOSS_DEV_FILENAME = f\"{SAVE_MODEL_DIR}/Losses_Dev_{EPOCHS}.json\"\n",
    "\n",
    "PAD_TOKEN_LABEL_ID = CrossEntropyLoss().ignore_index # -100\n",
    "\n",
    "if not os.path.exists(SAVE_MODEL_DIR):\n",
    "    os.makedirs(SAVE_MODEL_DIR)\n",
    "\n",
    "\n",
    "# Initialize Random seeds and validate if there's a GPU available...\n",
    "device, USE_CUDA = utils.get_torch_device(GPU_RUN_IX)\n",
    "random.seed(SEED_VAL)\n",
    "np.random.seed(SEED_VAL)\n",
    "torch.manual_seed(SEED_VAL)\n",
    "torch.cuda.manual_seed_all(SEED_VAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record everything inside a Log File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console_hdlr = logging.StreamHandler(sys.stdout)\n",
    "file_hdlr = logging.FileHandler(filename=f\"{SAVE_MODEL_DIR}/BERT_TokenClassifier_train_{EPOCHS}.log\")\n",
    "logging.basicConfig(level=logging.INFO, handlers=[console_hdlr, file_hdlr])\n",
    "logging.info(\"Start Logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME, do_basic_tokenize=False)\n",
    "\n",
    "# Load Train Dataset\n",
    "train_data, train_labels, train_label2index = utils.read_conll(TRAIN_DATA_PATH, has_labels=True)\n",
    "train_inputs, train_masks, train_labels, seq_lengths = utils.data_to_tensors(train_data, \n",
    "                                                                            tokenizer, \n",
    "                                                                            max_len=SEQ_MAX_LEN, \n",
    "                                                                            labels=train_labels, \n",
    "                                                                            label2index=train_label2index,\n",
    "                                                                            pad_token_label_id=PAD_TOKEN_LABEL_ID)\n",
    "utils.save_label_dict(train_label2index, filename=LABELS_FILENAME)\n",
    "index2label = {v: k for k, v in train_label2index.items()}\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load Dev Dataset\n",
    "dev_data, dev_labels, _ = utils.read_conll(DEV_DATA_PATH, has_labels=True)\n",
    "dev_inputs, dev_masks, dev_labels, dev_lens = utils.data_to_tensors(dev_data, \n",
    "                                                                    tokenizer, \n",
    "                                                                    max_len=SEQ_MAX_LEN, \n",
    "                                                                    labels=dev_labels, \n",
    "                                                                    label2index=train_label2index,\n",
    "                                                                    pad_token_label_id=PAD_TOKEN_LABEL_ID)\n",
    "\n",
    "# Create the DataLoader for our Development set.\n",
    "dev_data = TensorDataset(dev_inputs, dev_masks, dev_labels, dev_lens)\n",
    "dev_sampler = RandomSampler(dev_data)\n",
    "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(BERT_MODEL_NAME, num_labels=len(train_label2index))\n",
    "model.config.finetuning_task = 'token-classification'\n",
    "model.config.id2label = index2label\n",
    "model.config.label2id = train_label2index\n",
    "if USE_CUDA: model.cuda()\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "# Create optimizer and the learning rate scheduler.\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                        num_warmup_steps=0,\n",
    "                                        num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Cycle (Fine-tunning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_trn_values, loss_dev_values = [], []\n",
    "\n",
    "\n",
    "for epoch_i in range(1, EPOCHS+1):\n",
    "    # Perform one full pass over the training set.\n",
    "    logging.info(\"\")\n",
    "    logging.info('======== Epoch {:} / {:} ========'.format(epoch_i, EPOCHS))\n",
    "    logging.info('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Progress update\n",
    "        if step % PRINT_INFO_EVERY == 0 and step != 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = utils.format_time(time.time() - t0)\n",
    "            # Report progress.\n",
    "            logging.info('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.    Loss: {}.'.format(step, len(train_dataloader),\n",
    "                                                                                            elapsed, loss.item()))\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_trn_values.append(avg_train_loss)\n",
    "\n",
    "    logging.info(\"\")\n",
    "    logging.info(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n",
    "    logging.info(\"  Training Epoch took: {:}\".format(utils.format_time(time.time() - t0)))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on our validation set.\n",
    "    t0 = time.time()\n",
    "    results, preds_list = utils.evaluate_bert_model(dev_dataloader, BATCH_SIZE, model, tokenizer, index2label, PAD_TOKEN_LABEL_ID, prefix=\"Validation Set\")\n",
    "    loss_dev_values.append(results['loss'])\n",
    "    logging.info(\"  Validation Loss: {0:.2f}\".format(results['loss']))\n",
    "    logging.info(\"  Precision: {0:.2f} || Recall: {1:.2f} || F1: {2:.2f}\".format(results['precision']*100, results['recall']*100, results['f1']*100))\n",
    "    logging.info(\"  Validation took: {:}\".format(utils.format_time(time.time() - t0)))\n",
    "\n",
    "\n",
    "    # Save Checkpoint for this Epoch\n",
    "    utils.save_model(f\"{SAVE_MODEL_DIR}/EPOCH_{epoch_i}\", {\"args\":[]}, model, tokenizer)\n",
    "\n",
    "\n",
    "utils.save_losses(loss_trn_values, filename=LOSS_TRN_FILENAME)\n",
    "utils.save_losses(loss_dev_values, filename=LOSS_DEV_FILENAME)\n",
    "logging.info(\"\")\n",
    "logging.info(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Fine-tuned Model for Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from torch.utils.data import SequentialSampler\n",
    "\n",
    "GPU_IX=0\n",
    "device, USE_CUDA = utils.get_torch_device(GPU_IX)\n",
    "FILE_HAS_GOLD = True\n",
    "SEQ_MAX_LEN = 256\n",
    "BATCH_SIZE = 4\n",
    "# IMPORTANT NOTE: We predict on the dev set to make the results comparable with your previous models from this course\n",
    "TEST_DATA_PATH = \"data/trial_mini_data.conll\" # \"data/conll2003.dev.conll\"\n",
    "# TEST_DATA_PATH = \"data/trial_unk_data.conll\"\n",
    "MODEL_DIR = \"saved_models/MY_BERT_NER/\"\n",
    "LOAD_EPOCH = 1\n",
    "INPUTS_PATH=f\"{MODEL_DIR}/EPOCH_{LOAD_EPOCH}/model_inputs.txt\"\n",
    "OUTPUTS_PATH=f\"{MODEL_DIR}/EPOCH_{LOAD_EPOCH}/model_outputs.txt\"\n",
    "PAD_TOKEN_LABEL_ID = CrossEntropyLoss().ignore_index # -100\n",
    "\n",
    "console_hdlr = logging.StreamHandler(sys.stdout)\n",
    "file_hdlr = logging.FileHandler(filename=f\"{MODEL_DIR}/EPOCH_{LOAD_EPOCH}/BERT_TokenClassifier_predictions.log\")\n",
    "logging.basicConfig(level=logging.INFO, handlers=[console_hdlr, file_hdlr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = utils.load_model(BertForTokenClassification, BertTokenizer, f\"{MODEL_DIR}/EPOCH_{LOAD_EPOCH}\")\n",
    "label2index = utils.load_label_dict(f\"{MODEL_DIR}/label2index.json\")\n",
    "index2label = {v:k for k,v in label2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load File for Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_labels, _ = utils.read_conll(TEST_DATA_PATH, has_labels=FILE_HAS_GOLD)\n",
    "prediction_inputs, prediction_masks, gold_labels, seq_lens = utils.data_to_tensors(test_data, \n",
    "                                                                                   tokenizer, \n",
    "                                                                                   max_len=SEQ_MAX_LEN, \n",
    "                                                                                   labels=test_labels, \n",
    "                                                                                   label2index=label2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FILE_HAS_GOLD:\n",
    "    prediction_data = TensorDataset(prediction_inputs, prediction_masks, gold_labels, seq_lens)\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "    logging.info('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "    \n",
    "    results, preds_list = utils.evaluate_bert_model(prediction_dataloader, BATCH_SIZE, model, tokenizer, index2label, \n",
    "                                                        PAD_TOKEN_LABEL_ID, full_report=True, prefix=\"Test Set\")\n",
    "    logging.info(\"  Test Loss: {0:.2f}\".format(results['loss']))\n",
    "    logging.info(\"  Precision: {0:.2f} || Recall: {1:.2f} || F1: {2:.2f}\".format(results['precision']*100, results['recall']*100, results['f1']*100))\n",
    "\n",
    "    with open(OUTPUTS_PATH, \"w\") as fout:\n",
    "        with open(INPUTS_PATH, \"w\") as fin:\n",
    "            for sent, pred in preds_list:\n",
    "                fin.write(\" \".join(sent)+\"\\n\")\n",
    "                fout.write(\" \".join(pred)+\"\\n\")\n",
    "\n",
    "else:\n",
    "    # https://huggingface.co/transformers/main_classes/pipelines.html#transformers.TokenClassificationPipeline\n",
    "    logging.info('Predicting labels for {:,} test sentences...'.format(len(test_data)))\n",
    "    if not USE_CUDA: GPU_IX = -1\n",
    "    nlp = pipeline('token-classification', model=model, tokenizer=tokenizer, device=GPU_IX)\n",
    "    nlp.ignore_labels = []\n",
    "    with open(OUTPUTS_PATH, \"w\") as fout:\n",
    "        with open(INPUTS_PATH, \"w\") as fin:\n",
    "            for seq_ix, seq in enumerate(test_data):\n",
    "                sentence = \" \".join(seq)\n",
    "                predicted_labels = []\n",
    "                output_obj = nlp(sentence)\n",
    "                # [print(o) for o in output_obj]\n",
    "                for tok in output_obj:\n",
    "                    if '##' not in tok['word']:\n",
    "                        predicted_labels.append(tok['entity'])\n",
    "                logging.info(f\"\\n----- {seq_ix+1} -----\\n{seq}\\nPRED:{predicted_labels}\")\n",
    "                fin.write(sentence+\"\\n\")\n",
    "                fout.write(\" \".join(predicted_labels)+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f37b899a14b1e53256e3dbe85dea3859019f1cb8d1c44a9c4840877cfd0e7ef"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
